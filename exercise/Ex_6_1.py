from __future__ import division
import numpy as np


def cal_probability(train_data, train_label, test_data):
    sample_num = len(train_label)
    prob = {}
    for label in list(set(train_label)):
        prob_now = 1
        for i, element in enumerate(test_data):
            up = 0
            down = 0
            for j in range(sample_num):
                if train_label[j] == label:
                    down += 1
                    if train_data[i][j] == element:
                        up += 1
            prob_now *= up/down
        prob_now *= down / sample_num
        prob[label] = prob_now
    return prob


def cal_probability_smooth(train_data, train_label, test_data):
    sample_num = len(train_label)
    label_class = len(list(set(train_label)))
    prob = {}
    for label in list(set(train_label)):
        prob_now = 1
        for i, element in enumerate(test_data):
            up = 0
            down = 0
            feature_class = len(list(set(train_data[i])))
            for j in range(sample_num):
                if train_label[j] == label:
                    down += 1
                    if train_data[i][j] == element:
                        up += 1
            prob_now *= (up+1)/(down+feature_class)
        prob_now *= (down+1)/(sample_num+label_class)
        prob[label] = prob_now
    return prob


def main():
    train_data = [[1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3],
                  ['s', 'm', 'm', 's', 's', 's', 'm', 'm', 'l', 'l', 'l']]
    train_label = [-1, -1, 1, 1, -1, -1, -1, 1, 1, 1, 1]

    test_data = [1, 'm']
    prob = cal_probability(train_data, train_label, test_data)
    print('result generated by a naive Bayes classifier:')
    print(prob)
    print(max(prob, key=prob.get))

    prob_smooth = cal_probability_smooth(train_data, train_label, test_data)
    print('\nresult generated by a naive Bayes classifier with laplace smoothing:')
    print(prob_smooth)
    print(max(prob_smooth, key=prob_smooth.get))

if __name__ == '__main__':
    main()